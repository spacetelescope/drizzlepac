"""Code that evaluates the quality of the SVM products generated by the drizzlepac package.

The JSON files generated here can be converted directly into a Pandas DataFrame
using the syntax:

>>> import json
>>> import pandas as pd
>>> with open("<rootname>_astrometry_resids.json") as jfile:
>>>     resids = json.load(jfile)
>>> pdtab = pd.DataFrame(resids)

These DataFrames can then be concatenated using:

>>> allpd = pdtab.concat([pdtab2, pdtab3])

where 'pdtab2' and 'pdtab3' are DataFrames generated from other datasets.  For
more information on how to merge DataFrames, see

https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html

Visualization of these Pandas DataFrames with Bokeh can follow the example
from:

https://programminghistorian.org/en/lessons/visualizing-with-bokeh

"""

# Standard library imports
import argparse
import collections
from datetime import datetime
import glob
import json
import os
import pdb
import pickle
import sys
import time

# Related third party imports
from astropy.coordinates import SkyCoord
from astropy.io import ascii, fits
from astropy.stats import sigma_clipped_stats
from astropy.table import Table
from itertools import chain
import numpy as np
from scipy.spatial import KDTree

# Local application imports
from drizzlepac import util, wcs_functions
from drizzlepac.hlautils import hla_flag_filter
from drizzlepac.hlautils import astrometric_utils as au
import drizzlepac.hlautils.diagnostic_utils as du
import drizzlepac.devutils.comparison_tools.compare_sourcelists as csl
from drizzlepac.devutils.comparison_tools.read_hla import read_hla_catalog
from stsci.tools import logutil
from stwcs import wcsutil
from stwcs.wcsutil import HSTWCS

__taskname__ = 'svm_quality_analysis'

MSG_DATEFMT = '%Y%j%H%M%S'
SPLUNK_MSG_FORMAT = '%(asctime)s %(levelname)s src=%(name)s- %(message)s'
log = logutil.create_logger(__name__, level=logutil.logging.NOTSET, stream=sys.stdout,
                            format=SPLUNK_MSG_FORMAT, datefmt=MSG_DATEFMT)
# ----------------------------------------------------------------------------------------------------------------------


def characterize_gaia_distribution(hap_obj, json_timestamp=None, json_time_since_epoch=None,
                                   log_level=logutil.logging.NOTSET):
    """Statistically describe distribution of GAIA sources in footprint.

    Computes and writes the file to a json file:

    - Number of GAIA sources
    - X centroid location
    - Y centroid location
    - X offset of centroid from image center
    - Y offset of centroid from image center
    - X standard deviation
    - Y standard deviation
    - minimum closest neighbor distance
    - maximum closest neighbor distance
    - mean closest neighbor distance
    - standard deviation of closest neighbor distances

    Parameters
    ----------
    hap_obj : drizzlepac.hlautils.Product.FilterProduct
        hap product object to process

    json_timestamp: str, optional
        Universal .json file generation date and time (local timezone) that will be used in the instantiation
        of the HapDiagnostic object. Format: MM/DD/YYYYTHH:MM:SS (Example: 05/04/2020T13:46:35). If not
        specified, default value is logical 'None'

    json_time_since_epoch : float
        Universal .json file generation time that will be used in the instantiation of the HapDiagnostic
        object. Format: Time (in seconds) elapsed since January 1, 1970, 00:00:00 (UTC). If not specified,
        default value is logical 'None'

    log_level : int, optional
        The desired level of verboseness in the log statements displayed on the screen and written to the
        .log file. Default value is 'NOTSET'.

    Returns
    -------
    Nothing
    """
    log.setLevel(log_level)

    # get table of GAIA sources in footprint
    gaia_table = generate_gaia_catalog(hap_obj, columns_to_remove=['mag', 'objID', 'GaiaID'])

    # if log_level is either 'DEBUG' or 'NOTSET', write out GAIA sources to DS9 region file
    if log_level <= logutil.logging.DEBUG:
        reg_file = "{}_gaia_sources.reg".format(hap_obj.drizzle_filename[:-9])
        gaia_table.write(reg_file, format='ascii.csv')
        log.debug("Wrote GAIA source RA and Dec positions to DS9 region file '{}'".format(reg_file))

    # convert RA, Dec to image X, Y
    outwcs = HSTWCS(hap_obj.drizzle_filename + "[1]")
    x, y = outwcs.all_world2pix(gaia_table['RA'], gaia_table['DEC'], 1)

    # compute stats for the distribution
    centroid = [np.mean(x), np.mean(y)]
    centroid_offset = []
    for idx in range(0, 2):
        centroid_offset.append(outwcs.wcs.crpix[idx] - centroid[idx])
    std_dev = [np.std(x), np.std(y)]

    # Find straight-line distance to the closest neighbor for each GAIA source
    xys = np.array([x, y])
    xys = xys.reshape(len(x), 2)
    tree = KDTree(xys)
    neighborhood = tree.query(xys, 2)
    min_seps = np.empty([0])
    for sep_pair in neighborhood[0]:
        min_seps = np.append(min_seps, sep_pair[1])

    # add statistics to out_dict
    out_dict = collections.OrderedDict()
    out_dict["units"] = "pixels"
    out_dict["Number of GAIA sources"] = len(gaia_table)
    axis_list = ["X", "Y"]
    title_list = ["centroid", "offset of centroid from image center", "standard deviation"]
    for item_value, item_title in zip([centroid, centroid_offset, std_dev], title_list):
        for axis_item in enumerate(axis_list):
            log.info("{} {} ({}): {}".format(axis_item[1], item_title, out_dict["units"],
                                             item_value[axis_item[0]]))
            out_dict["{} {}".format(axis_item[1], item_title)] = item_value[axis_item[0]]
    min_sep_stats = [min_seps.min(), min_seps.max(), min_seps.mean(), min_seps.std()]
    min_sep_title_list = ["minimum closest neighbor distance",
                          "maximum closest neighbor distance",
                          "mean closest neighbor distance",
                          "standard deviation of closest neighbor distances"]
    for item_value, item_title in zip(min_sep_stats, min_sep_title_list):
        log.info("{} ({}): {}".format(item_title, out_dict["units"], item_value))
        out_dict[item_title] = item_value

    # write catalog to HapDiagnostic-formatted .json file.
    diag_obj = du.HapDiagnostic(log_level=log_level)
    diag_obj.instantiate_from_hap_obj(hap_obj,
                                      data_source="{}.characterize_gaia_distribution".format(__taskname__),
                                      description="A statistical characterization of the distribution of "
                                                  "GAIA sources in image footprint",
                                      timestamp=json_timestamp, time_since_epoch=json_time_since_epoch)
    diag_obj.add_data_item(out_dict, "distribution characterization statistics")
    diag_obj.write_json_file(hap_obj.drizzle_filename[:-9] + "_svm_gaia_distribution_characterization.json",
                             clobber=True)


# ----------------------------------------------------------------------------------------------------------------------

def compare_num_sources(catalog_list, drizzle_list, json_timestamp=None, json_time_since_epoch=None,
                        log_level=logutil.logging.NOTSET):
    """Determine the number of viable sources actually listed in SVM output catalogs.

    Parameters
    ----------
    catalog_list: list of strings
        Set of files on which to actually perform comparison.  Catalogs, Point and
        Segment, are generated for all of the Total data products in a single visit.
        The catalogs are detector-dependent.

    drizzle_list: list of strings
        Drizzle files for the Total products which were mined to generate the output catalogs.

    json_timestamp: str, optional
        Universal .json file generation date and time (local timezone) that will be used in the instantiation
        of the HapDiagnostic object. Format: MM/DD/YYYYTHH:MM:SS (Example: 05/04/2020T13:46:35). If not
        specified, default value is logical 'None'

    json_time_since_epoch : float
        Universal .json file generation time that will be used in the instantiation of the HapDiagnostic
        object. Format: Time (in seconds) elapsed since January 1, 1970, 00:00:00 (UTC). If not specified,
        default value is logical 'None'

    log_level : int, optional
        The desired level of verboseness in the log statements displayed on the screen and written to the
        .log file. Default value is 'NOTSET'.

    .. note:: This routine can be run either as a direct call from the hapsequencer.py routine,
    or it can invoked by a simple Python driver (or from within a Python session) by providing
    the names of the previously computed files as lists. The files must exist in the working directory.
    """
    log.setLevel(log_level)

    pnt_suffix = '_point-cat.ecsv'
    seg_suffix = '_segment-cat.ecsv'

    # Generate a separate JSON file for each detector
    # Drizzle filename example: hst_11665_06_wfc3_ir_total_ib4606_drz.fits
    # The filename is all lower-case by design.
    for drizzle_file in drizzle_list:
        tokens = drizzle_file.split('_')
        detector = tokens[4]
        ipppss = tokens[6]

        sources_dict = {'detector': detector, 'point': 0, 'segment': 0}

        # Construct the output JSON filename
        json_filename = ipppss + '_' + detector + '_svm_num_sources.json'

        # Construct catalog names for catalogs that should have been produced
        prefix = '_'.join(tokens[0:-1])
        cat_names = [prefix + pnt_suffix, prefix + seg_suffix]

        # If the catalog were actually produced, get the number of sources.
        # A catalog may not be produced because it was not requested, or there
        # was an error.  However, for the purposes of this program, it is OK
        # that no catalog was produced.
        for catalog in cat_names:
            does_exist = any(catalog in x for x in catalog_list)

            # if the catalog exists, open it and find the number of sources string
            num_sources = -1
            cat_type = ""
            if does_exist:
                file = open(catalog, 'r')
                for line in file:
                    sline = line.strip()

                    # All the comments are grouped at the start of the file. When
                    # the first non-comment line is found, there is no need to look further.
                    if not sline.startswith('#'):
                        log.info("Number of sources not reported in Catalog: {}.".format(catalog))
                        break

                    # When the matching comment line is found, get the value.
                    if sline.find('Number of sources') != -1:
                        num_sources = sline.split(' ')[-1][0:-1]
                        log.info("Catalog: {} Number of sources: {}.".format(catalog, num_sources))
                        break

                cat_type = 'point' if catalog.find("point") != -1 else 'segment'
                sources_dict[cat_type] = int(num_sources)

        # Set up the diagnostic object and write out the results
        diagnostic_obj = du.HapDiagnostic()
        diagnostic_obj.instantiate_from_fitsfile(drizzle_file,
                                                 data_source="{}.compare_num_sources".format(__taskname__),
                                                 description="Number of sources in Point and Segment "
                                                             "catalogs",
                                                 timestamp=json_timestamp,
                                                 time_since_epoch=json_time_since_epoch)
        diagnostic_obj.add_data_item(sources_dict, 'number_of_sources')
        diagnostic_obj.write_json_file(json_filename)
        log.info("Generated quality statistics (number of sources) as {}.".format(json_filename))

        # Clean up
        del diagnostic_obj

# ------------------------------------------------------------------------------------------------------------


def compare_ra_dec_crossmatches(hap_obj, json_timestamp=None, json_time_since_epoch=None,
                                log_level=logutil.logging.NOTSET):
    """Compare the equatorial coordinates of cross-matches sources between the Point and Segment catalogs.
    The results .json file contains the following information:

        - image header information
        - cross-match details (input catalog lengths, number of cross-matched sources, coordinate system)
        - catalog containing RA and dec values of cross-matched point catalog sources
        - catalog containing RA and dec values of cross-matched segment catalog sources
        - Statistics describing the on-sky separation of the cross-matched point and segment catalogs
        (non-clipped and sigma-clipped mean, median and standard deviation values)

    Parameters
    ----------
    hap_obj : drizzlepac.hlautils.Product.FilterProduct
        hap filter product object to process

    json_timestamp: str, optional
        Universal .json file generation date and time (local timezone) that will be used in the instantiation
        of the HapDiagnostic object. Format: MM/DD/YYYYTHH:MM:SS (Example: 05/04/2020T13:46:35). If not
        specified, default value is logical 'None'

    json_time_since_epoch : float
        Universal .json file generation time that will be used in the instantiation of the HapDiagnostic
        object. Format: Time (in seconds) elapsed since January 1, 1970, 00:00:00 (UTC). If not specified,
        default value is logical 'None'

    log_level : int, optional
        The desired level of verboseness in the log statements displayed on the screen and written to the
        .log file. Default value is 'NOTSET'.

    Returns
    --------
    nothing.
    """
    log.setLevel(log_level)
    sl_names = [hap_obj.point_cat_filename, hap_obj.segment_cat_filename]
    img_names = [hap_obj.drizzle_filename, hap_obj.drizzle_filename]
    good_flag_sum = 255  # all bits good

    diag_obj = du.HapDiagnostic(log_level=log_level)
    diag_obj.instantiate_from_hap_obj(hap_obj,
                                      data_source="{}.compare_ra_dec_crossmatches".format(__taskname__),
                                      description="matched point and segment catalog RA and Dec values",
                                      timestamp=json_timestamp,
                                      time_since_epoch=json_time_since_epoch)
    json_results_dict = collections.OrderedDict()
    # add reference and comparision catalog filenames as header elements
    json_results_dict["point catalog filename"] = sl_names[0]
    json_results_dict["segment catalog filename"] = sl_names[1]

    # 1: Read in sourcelists files into astropy table or 2-d array so that individual columns from each
    # sourcelist can be easily accessed later in the code.
    point_data, seg_data = csl.slFiles2dataTables(sl_names)
    log.info("Valid point data columns:   {}".format(list(point_data.keys())))
    log.info("Valid segment data columns: {}".format(list(seg_data.keys())))
    log.info("\n")
    log.info("Data columns to be compared:")
    columns_to_compare = list(set(point_data.keys()).intersection(set(seg_data.keys())))
    for listItem in sorted(columns_to_compare):
        log.info(listItem)
    log.info("\n")
    # 2: Run starmatch_hist to get list of matched sources common to both input sourcelists
    sl_lengths = [len(point_data['RA']), len(seg_data['RA'])]
    json_results_dict['point catalog length'] = sl_lengths[0]
    json_results_dict['segment catalog length'] = sl_lengths[1]
    matching_lines_ref, matching_lines_img = csl.getMatchedLists(sl_names, img_names, sl_lengths,
                                                                 log_level=log_level)
    json_results_dict['number of cross-matches'] = len(matching_lines_ref)

    # Report number and percentage of the total number of detected ref and comp sources that were matched
    log.info("Cross-matching results")
    log.info(
        "Point sourcelist:  {} of {} total sources cross-matched ({}%)".format(len(matching_lines_ref),
                                                                               sl_lengths[0],
                                                                               100.0 *
                                                                               (float(len(matching_lines_ref))
                                                                                / float(sl_lengths[0]))))
    log.info(
        "Segment sourcelist: {} of {} total sources cross-matched ({}%)".format(len(matching_lines_img),
                                                                                sl_lengths[1],
                                                                                100.0 *
                                                                                (float(
                                                                                    len(matching_lines_img))
                                                                                 / float(sl_lengths[1]))))
    # return without creating a .json if no cross-matches are found
    if len(matching_lines_ref) == 0 or len(matching_lines_img) == 0:
        log.error("*** No matching sources were found. Comparisons cannot be computed. "
                  "No json file will be produced.***")
        return
    # 2: Create masks to remove missing values or values not considered "good" according to user-specified
    # good bit values
    # 2a: create mask that identifies lines any value from any column is missing
    missing_mask = csl.mask_missing_values(point_data, seg_data, matching_lines_ref, matching_lines_img,
                                           columns_to_compare)
    # 2b: create mask based on flag values
    matched_values = csl.extractMatchedLines("FLAGS", point_data, seg_data, matching_lines_ref,
                                             matching_lines_img)
    bitmask = csl.make_flag_mask(matched_values, good_flag_sum, missing_mask)

    matched_values_ra = csl.extractMatchedLines("RA", point_data, seg_data, matching_lines_ref,
                                                matching_lines_img, bitmask=bitmask)
    matched_values_dec = csl.extractMatchedLines("DEC", point_data, seg_data, matching_lines_ref,
                                                 matching_lines_img, bitmask=bitmask)

    if matched_values_ra.shape[1] > 0 and matched_values_ra.shape[1] == matched_values_dec.shape[1]:
        # get coordinate system type from fits headers

        point_frame = fits.getval(img_names[0], "radesys", ext=('sci', 1)).lower()
        seg_frame = fits.getval(img_names[1], "radesys", ext=('sci', 1)).lower()
        # Add 'ref_frame' and 'comp_frame" values to header so that will SkyCoord() execute OK
        json_results_dict["point frame"] = point_frame
        json_results_dict["segment frame"] = seg_frame

        # convert reference and comparision RA/Dec values into SkyCoord objects
        matched_values_point = SkyCoord(matched_values_ra[0, :], matched_values_dec[0, :], frame=point_frame,
                                        unit="deg")
        matched_values_seg = SkyCoord(matched_values_ra[1, :], matched_values_dec[1, :], frame=seg_frame,
                                      unit="deg")
        # convert to ICRS coord system
        if point_frame != "icrs":
            matched_values_point = matched_values_point.icrs
        if seg_frame != "icrs":
            matched_values_seg = matched_values_seg.icrs

        # compute on-sky separations in arcseconds
        sep = matched_values_seg.separation(matched_values_point).arcsec

        # Compute and store statistics  on separations
        sep_stat_dict = collections.OrderedDict()
        sep_stat_dict["units"] = "arcseconds"
        sep_stat_dict["Non-clipped min"] = np.min(sep)
        sep_stat_dict["Non-clipped max"] = np.max(sep)
        sep_stat_dict["Non-clipped mean"] = np.mean(sep)
        sep_stat_dict["Non-clipped median"] = np.median(sep)
        sep_stat_dict["Non-clipped standard deviation"] = np.std(sep)
        sigma = 3
        maxiters = 3
        clipped_stats = sigma_clipped_stats(sep, sigma=sigma, maxiters=maxiters)
        sep_stat_dict["{}x{} sigma-clipped mean".format(maxiters, sigma)] = clipped_stats[0]
        sep_stat_dict["{}x{} sigma-clipped median".format(maxiters, sigma)] = clipped_stats[1]
        sep_stat_dict["{}x{} sigma-clipped standard deviation".format(maxiters, sigma)] = clipped_stats[2]

        # Create output catalogs for json file
        out_cat_point = Table([matched_values_ra[0], matched_values_dec[0]],
                              names=("Right ascension", "Declination"))
        out_cat_seg = Table([matched_values_ra[1], matched_values_dec[1]],
                            names=("Right ascension", "Declination"))
        for table_item in [out_cat_point, out_cat_seg]:
            for col_name in ["Right ascension", "Declination"]:
                table_item[col_name].unit = "degrees"  # Add correct units

        # add various data items to diag_obj
        diag_obj.add_data_item(json_results_dict, "Cross-match details")
        diag_obj.add_data_item(out_cat_point, "Cross-matched point catalog")
        diag_obj.add_data_item(out_cat_seg, "Cross-matched segment catalog")
        diag_obj.add_data_item(sep_stat_dict, "Segment - point on-sky separation statistics")

        # write everything out to the json file
        json_filename = hap_obj.drizzle_filename[:-9]+"_svm_point_segment_crossmatch.json"
        diag_obj.write_json_file(json_filename, clobber=True)
    else:
        log.warning("Point vs. segment catalog cross match test could not be performed.")

# ------------------------------------------------------------------------------------------------------------


def find_gaia_sources(hap_obj, json_timestamp=None, json_time_since_epoch=None,
                      log_level=logutil.logging.NOTSET):
    """Creates a catalog of all GAIA sources in the footprint of a specified HAP final product image, and
    stores the GAIA object catalog as a hap diagnostic json file. The catalog contains RA, Dec and magnitude
    of each identified source. The catalog is sorted in descending order by brightness.

    Parameters
    ----------
    hap_obj : drizzlepac.hlautils.Product.TotalProduct, drizzlepac.hlautils.Product.FilterProduct, or
        drizzlepac.hlautils.Product.ExposureProduct, depending on input.
        hap product object to process

    json_timestamp: str, optional
        Universal .json file generation date and time (local timezone) that will be used in the instantiation
        of the HapDiagnostic object. Format: MM/DD/YYYYTHH:MM:SS (Example: 05/04/2020T13:46:35). If not
        specified, default value is logical 'None'

    json_time_since_epoch : float
        Universal .json file generation time that will be used in the instantiation of the HapDiagnostic
        object. Format: Time (in seconds) elapsed since January 1, 1970, 00:00:00 (UTC). If not specified,
        default value is logical 'None'

    log_level : int, optional
        The desired level of verboseness in the log statements displayed on the screen and written to the
        .log file. Default value is 'NOTSET'.

    Returns
    -------
    Nothing.
    """
    log.setLevel(log_level)
    gaia_table = generate_gaia_catalog(hap_obj, columns_to_remove=['objID', 'GaiaID'])
    # write catalog to HapDiagnostic-formatted .json file.
    diag_obj = du.HapDiagnostic(log_level=log_level)
    diag_obj.instantiate_from_hap_obj(hap_obj,
                                      data_source="{}.find_gaia_sources".format(__taskname__),
                                      description="A table of GAIA sources in image footprint",
                                      timestamp=json_timestamp,
                                      time_since_epoch=json_time_since_epoch)
    diag_obj.add_data_item(gaia_table, "GAIA sources")  # write catalog of identified GAIA sources
    diag_obj.add_data_item(len(gaia_table), "Number of GAIA sources")  # write the number of GAIA sources
    diag_obj.write_json_file(hap_obj.drizzle_filename[:-9]+"_svm_gaia_sources.json", clobber=True)

    # Clean up
    del diag_obj
    del gaia_table

# ----------------------------------------------------------------------------------------------------------------------


def generate_gaia_catalog(hap_obj, columns_to_remove=None):
    """Uses astrometric_utils.create_astrometric_catalog() to create a catalog of all GAIA sources in the
    image footprint. This catalog contains right ascension, declination, and magnitude values, and is sorted
    in descending order by brightness.

    Parameters
    ----------
    hap_obj : drizzlepac.hlautils.Product.TotalProduct, drizzlepac.hlautils.Product.FilterProduct, or
        drizzlepac.hlautils.Product.ExposureProduct, depending on input.
        hap product object to process

    columns_to_remove : list
        list of columns to remove from the table of GAIA sources returned by this function

    Returns
    -------
    gaia_table : astropy table
        table containing right ascension, declination, and magnitude of all GAIA sources identified in the
        image footprint, sorted in descending order by brightness.
    """
    # Gather list of input flc/flt images
    img_list = []
    log.debug("GAIA catalog will be created using the following input images:")
    # Create a list of the input flc.fits/flt.fits that were drizzled to create the final HAP product being
    # processed here. edp_item.info and hap_obj.info are both structured as follows:
    # <proposal id>_<visit #>_<instrument>_<detector>_<input filename>_<filter>_<drizzled product
    # image filetype>
    # Example: '10265_01_acs_wfc_j92c01b9q_flc.fits_f606w_drc'
    # what is being extracted here is just the input filename, which in this case is 'j92c01b9q_flc.fits'.
    if hasattr(hap_obj, "edp_list"):  # for total and filter product objects
        for edp_item in hap_obj.edp_list:
            parse_info = edp_item.info.split("_")
            imgname = "{}_{}".format(parse_info[4], parse_info[5])
            log.debug(imgname)
            img_list.append(imgname)
    else:  # For single-exposure product objects
        parse_info = hap_obj.info.split("_")
        imgname = "{}_{}".format(parse_info[4], parse_info[5])
        log.debug(imgname)
        img_list.append(imgname)

    # generate catalog of GAIA sources
    gaia_table = au.create_astrometric_catalog(img_list, gaia_only=True, use_footprint=True)

    # trim off specified columns
    if columns_to_remove:
        gaia_table.remove_columns(columns_to_remove)

    # remove sources outside image footprint
    outwcs = wcsutil.HSTWCS(hap_obj.drizzle_filename, ext=1)
    x, y = outwcs.all_world2pix(gaia_table['RA'], gaia_table['DEC'], 1)
    imghdu = fits.open(hap_obj.drizzle_filename)
    in_img_data = imghdu['WHT'].data.copy()
    in_img_data = np.where(in_img_data == 0, np.nan, in_img_data)
    mask = au.within_footprint(in_img_data, outwcs, x, y)
    gaia_table = gaia_table[mask]

    # Report results to log
    if len(gaia_table) == 0:
        log.warning("No GAIA sources were found!")
    elif len(gaia_table) == 1:
        log.info("1 GAIA source was found.")
    else:
        log.info("{} GAIA sources were found.".format(len(gaia_table)))
    return gaia_table

# ----------------------------------------------------------------------------------------------------------------------


def compare_photometry(drizzle_list, json_timestamp=None, json_time_since_epoch=None,
                       log_level=logutil.logging.NOTSET):
    """Compare photometry measurements for sources cross matched between the Point and Segment catalogs.

    Parameters
    ----------
    drizzle_list: list of strings
        Drizzle files for the Filter products which were mined to generate the output catalogs.

    json_timestamp: str, optional
        Universal .json file generation date and time (local timezone) that will be used in the instantiation
        of the HapDiagnostic object. Format: MM/DD/YYYYTHH:MM:SS (Example: 05/04/2020T13:46:35). If not
        specified, default value is logical 'None'

    json_time_since_epoch : float
        Universal .json file generation time that will be used in the instantiation of the HapDiagnostic
        object. Format: Time (in seconds) elapsed since January 1, 1970, 00:00:00 (UTC). If not specified,
        default value is logical 'None'

    log_level : int, optional
        The desired level of verboseness in the log statements displayed on the screen and written to the
        .log file. Default value is 'NOTSET'.

    .. note:: This routine can be run either as a direct call from the hapsequencer.py routine,
    or it can invoked by a simple Python driver (or from within a Python session) by providing
    the names of the previously computed files as lists. The files must exist in the working directory.
    """
    log.setLevel(log_level)

    pnt_suffix = '_point-cat.ecsv'
    seg_suffix = '_segment-cat.ecsv'

    good_flag_sum = 255

    phot_column_names = ["MagAp1", "MagAp2"]
    error_column_names = ["MagErrAp1", "MagErrAp2"]

    # Generate a separate JSON file for each detector and filter product
    # Drizzle filename example: hst_11665_06_wfc3_ir_f110w_ib4606_drz.fits.
    # The "product" in this context is a filter name.
    # The filename is all lower-case by design.
    for drizzle_file in drizzle_list:
        tokens = drizzle_file.split('_')
        detector = tokens[4]
        filter_name = tokens[5]
        ipppss = tokens[6]

        # Set up the diagnostic object
        diagnostic_obj = du.HapDiagnostic()
        diagnostic_obj.instantiate_from_fitsfile(drizzle_file,
                                                 data_source="{}.compare_photometry".format(__taskname__),
                                                 description="Photometry differences in Point and "
                                                             "Segment catalogs",
                                                 timestamp=json_timestamp,
                                                 time_since_epoch=json_time_since_epoch)
        summary_dict = {'detector': detector, 'filter_name': filter_name}

        # Construct the output JSON filename
        json_filename = '_'.join([ipppss, detector, 'svm', filter_name, 'photometry.json'])

        # Construct catalog names for catalogs that should have been produced
        # For any drizzled product, only two catalogs can be produced at most (point and segment).
        prefix = '_'.join(tokens[0:-1])
        cat_names = [prefix + pnt_suffix, prefix + seg_suffix]

        # Check that both catalogs exist
        for catalog in cat_names:
            does_exist = os.path.isfile(catalog)
            if not does_exist:
                log.warning("Catalog {} does not exist.  Both the Point and Segment catalogs must exist "
                            "for comparison.".format(catalog))
                log.warning("Program skipping comparison of catalogs associated "
                            "with {}.\n".format(drizzle_file))
                continue

        # If the catalogs were actually produced, then get the data.
        tab_point_measurements = ascii.read(cat_names[0])
        tab_seg_measurements = ascii.read(cat_names[1])

        # Unfortunately the Point and Segment catalogs use different names for the X and Y values
        # Point: ([X|Y]-Center)  Segment: ([X|Y]-Centroid. Reset the coordinate columns to be only X or Y.
        tab_point_measurements.rename_column('X-Center', 'X')
        tab_point_measurements.rename_column('Y-Center', 'Y')
        tab_seg_measurements.rename_column('X-Centroid', 'X')
        tab_seg_measurements.rename_column('Y-Centroid', 'Y')
        cat_lengths = [len(tab_point_measurements), len(tab_seg_measurements)]

        # Determine the column names common to both catalogs as a list
        common_columns = list(set(tab_point_measurements.colnames).intersection(
            set(tab_seg_measurements.colnames)))

        # Use the utilities in devutils to match the sources in the two lists - get
        # the indices of the matches.
        matches_point_to_seg, matches_seg_to_point = csl.getMatchedLists(cat_names,
                                                                         [drizzle_file,
                                                                          drizzle_file],
                                                                         cat_lengths,
                                                                         log_level=log_level)
        if len(matches_point_to_seg) == 0 or len(matches_seg_to_point) == 0:
            log.warning("Catalog {} and Catalog {} had no matching sources.".format(cat_names[0],
                                                                                    cat_names[1]))
            log.warning("Program skipping comparison of catalog indices associated "
                        "with {}.\n".format(drizzle_file))
            continue

        # There are nan values present in the catalogs - create a mask which identifies these rows
        # which are missing valid data
        missing_values_mask = csl.mask_missing_values(tab_point_measurements, tab_seg_measurements,
                                                      matches_point_to_seg, matches_seg_to_point,
                                                      common_columns)

        # Extract the Flag column from the two catalogs and get an ndarray (2, length)
        flag_matching = csl.extractMatchedLines('Flags', tab_point_measurements, tab_seg_measurements,
                                                matches_point_to_seg, matches_seg_to_point)

        # Generate a mask to accommodate the missing, as well as the "flagged" entries
        flag_values_mask = csl.make_flag_mask(flag_matching, good_flag_sum, missing_values_mask)

        # Extract the columns of interest from the two catalogs for each desired measurement
        # and get an ndarray (2, length)
        # array([[21.512, ..., 2.944], [21.6 , ..., 22.98]],
        #       [[21.872, ..., 2.844], [21.2 , ..., 22.8]])
        for index, phot_column_name in enumerate(phot_column_names):
            matching_phot_rows = csl.extractMatchedLines(phot_column_name, tab_point_measurements,
                                                         tab_seg_measurements, matches_point_to_seg,
                                                         matches_seg_to_point, bitmask=flag_values_mask)

            # Compute the differences (Point - Segment)
            delta_phot = np.subtract(matching_phot_rows[0], matching_phot_rows[1])

            # Compute some basic statistics: mean difference and standard deviation, median difference,
            median_delta_phot = np.median(delta_phot)
            mean_delta_phot = np.mean(delta_phot)
            std_delta_phot = np.std(delta_phot)

            # NEED A BETTER WAY TO ASSOCIATE THE ERRORS WITH THE MEASUREMENTS
            # Compute the corresponding error of the differences
            matching_error_rows = csl.extractMatchedLines(error_column_names[index],
                                                          tab_point_measurements, tab_seg_measurements,
                                                          matches_point_to_seg, matches_seg_to_point,
                                                          bitmask=flag_values_mask)

            # Compute the error of the delta value (square root of the sum of the squares)
            result_error = np.sqrt(np.add(np.square(matching_error_rows[0]),
                                          np.square(matching_error_rows[1])))

            stat_key = 'Stats for Delta_' + phot_column_name + ' = Point_' + phot_column_name + \
                       ' - Segment_' + phot_column_name
            stat_dict = {stat_key: {'Mean Difference': mean_delta_phot, 'Standard Deviation': std_delta_phot,
                         'Median Difference': median_delta_phot}}
            summary_dict.update(stat_dict)

            # Write out the results
            diagnostic_obj.add_data_item(summary_dict,
                                         'High-level Photometry Statistics on differences of Point - Segment')

        diagnostic_obj.write_json_file(json_filename)
        log.info("Generated photometry comparison for Point - Segment matches "
                 "sources {}.".format(json_filename))

        # Clean up
        del diagnostic_obj

# ----------------------------------------------------------------------------------------------------------------------


def report_wcs(total_product_list, json_timestamp=None, json_time_since_epoch=None,
               log_level=logutil.logging.NOTSET):
    """Report the WCS information for each exposure of a total data product.

    Parameters
    ----------
    total_product_list: list of HAP TotalProduct objects, one object per instrument detector
    (drizzlepac.hlautils.Product.TotalProduct)

    json_timestamp: str, optional
        Universal .json file generation date and time (local timezone) that will be used in the instantiation
        of the HapDiagnostic object. Format: MM/DD/YYYYTHH:MM:SS (Example: 05/04/2020T13:46:35). If not
        specified, default value is logical 'None'

    json_time_since_epoch : float
        Universal .json file generation time that will be used in the instantiation of the HapDiagnostic
        object. Format: Time (in seconds) elapsed since January 1, 1970, 00:00:00 (UTC). If not specified,
        default value is logical 'None'

    log_level : int, optional
        The desired level of verboseness in the log statements displayed on the screen and
        written to the .log file.  Default value is 'NOTSET'.
    """
    log.setLevel(log_level)

    # Generate a separate JSON file for each total product (a total product
    # consists of a single detector.  Each total product consists of one or
    # more ExposureProduct objects.
    for total_product in total_product_list:
        detector = total_product.detector
        ipppss = total_product.edp_list[0].exposure_name[0:6]

        # Set up the diagnostic object
        diagnostic_obj = du.HapDiagnostic()
        diagnostic_obj.instantiate_from_hap_obj(total_product,
                                                data_source="{}.report_wcs".format(__taskname__),
                                                description="WCS information",
                                                timestamp=json_timestamp,
                                                time_since_epoch=json_time_since_epoch)

        # Construct the output JSON filename
        json_filename = '_'.join([ipppss, detector, 'svm_wcs.json'])

        # Loop over all the individual exposures in the list which comprise the total product
        for edp_object in total_product.edp_list:
            edp_filter = edp_object.filters

            # For exposures with multiple science extensions (multiple chips),
            # generate a combined WCS
            num_sci_ext, extname = util.count_sci_extensions(edp_object.full_filename)
            extname_list = []
            for x in range(num_sci_ext):
                extname_list.append((extname, x+1))

            metawcs = wcs_functions.make_mosaic_wcs(edp_object.full_filename)

            # Get information from the active WCS
            active_wcs_dict = {'primary_wcsname': metawcs.wcs.name,
                               'wcs_info': {'crpix1': metawcs.wcs.crpix[0],
                               'crpix2': metawcs.wcs.crpix[1],
                               'crval1': metawcs.wcs.crval[0], 'crval2': metawcs.wcs.crval[1],
                               'scale': metawcs.pscale, 'orientation': metawcs.orientat,
                               'exposure': edp_object.exposure_name}}

            diagnostic_obj.add_data_item(active_wcs_dict, 'PrimaryWCS_' + edp_object.exposure_name)

            # Determine the possible alternate WCS solutions in the header
            dict_of_wcskeys_names = wcsutil.altwcs.wcsnames(edp_object.full_filename, ext=1)

            # Ignore the OPUS ("O") WCS, as well as the duplicate of the active WCS
            dict_of_wcskeys_names.pop('O')

            reverse_dict = {}
            for key, value in dict_of_wcskeys_names.items():
                reverse_dict.setdefault(value, set()).add(key)
            keys_with_dups = set(chain.from_iterable(values for key, values in reverse_dict.items() if len(values) > 1))

            # Make a list of the keys which contain duplicate values...
            if keys_with_dups:
                list_keys = list(keys_with_dups)
                # ...ignore the primary key as it is important, and...
                list_keys.remove(' ')
                # ...remove the duplicates.
                for popkey in list_keys:
                    dict_of_wcskeys_names.pop(popkey)

            # The remaining dictionary items all need to share the same IDC base
            # solution as the "active" solution in order to make a consistent comparison -
            # remove any outliers.
            loc = metawcs.wcs.name.find("_")
            root_idc = metawcs.wcs.name[0:loc]

            bad_match_key = []
            for key, value in dict_of_wcskeys_names.items():
                if root_idc in value:
                    continue
                else:
                    bad_match_key.append(key)

            for bad_key in bad_match_key:
                dict_of_wcskeys_names.pop(bad_key)

            log.info("Remaining WCS keys and names based on same reference root {}".format(dict_of_wcskeys_names))

            # If there is anything left to compare, then do it.
            if len(dict_of_wcskeys_names) > 1:

                # Activate an alternate WCS in order to gather its information.
                # First copy the original primary WCS to an alternate (in case there was
                # not already a duplicate). Use key 'Z'.  *** FIX MDD Should check for Z in use.
                wcsutil.altwcs.archiveWCS(edp_object.full_filename, ext=extname_list, wcskey='Z')

                icnt = 0
                # Restore an alternate to be the primary WCS
                for key, value in dict_of_wcskeys_names.items():
                    if key != ' ':
                        wcsutil.altwcs.restoreWCS(edp_object.full_filename, ext=extname_list, wcskey=key)
                        alt_key = key
                        alt_wcs_name = value

                        # Create a metawcs for this alternate WCS
                        alt_metawcs = wcs_functions.make_mosaic_wcs(edp_object.full_filename)

                        # Get information from the alternate/active WCS
                        alt_wcs_dict = {'alternate_wcsname': alt_wcs_name,
                                        'wcs_info': {'crpix1': alt_metawcs.wcs.crpix[0],
                                        'crpix2': alt_metawcs.wcs.crpix[1],
                                        'crval1': alt_metawcs.wcs.crval[0], 'crval2': alt_metawcs.wcs.crval[1],
                                        'scale': alt_metawcs.pscale, 'orientation': alt_metawcs.orientat,
                                        'exposure': edp_object.exposure_name}}

                        diagnostic_obj.add_data_item(alt_wcs_dict, 'AlternateWCS_' + edp_object.exposure_name + '_' + str(icnt))

                        delta_wcs = metawcs.wcs.name + '-' + alt_wcs_name
                        diff_wcs_dict = {'delta_wcsname': delta_wcs, 'wcs_info': {}}
                        # if or wcs_key in active_wcs_dict.keys():
                        #    if wcs_key.find('wcsname') != -1:
                        #        continue
                        #    print("wcs_key: {}".format(wcs_key))
                        #    diff_wcs_dict['wcs_info'][wcs_key] = active_wcs_dict['wcs_info'][wcs_key] - alt_wcs_dict['wcs_info'][wcs_key]
                        diff_wcs_dict['wcs_info']['d_crpix1'] = active_wcs_dict['wcs_info']['crpix1'] - alt_wcs_dict['wcs_info']['crpix1']
                        diff_wcs_dict['wcs_info']['d_crpix2'] = active_wcs_dict['wcs_info']['crpix2'] - alt_wcs_dict['wcs_info']['crpix2']
                        diff_wcs_dict['wcs_info']['d_crval1'] = active_wcs_dict['wcs_info']['crval1'] - alt_wcs_dict['wcs_info']['crval1']
                        diff_wcs_dict['wcs_info']['d_crval2'] = active_wcs_dict['wcs_info']['crval2'] - alt_wcs_dict['wcs_info']['crval2']
                        diff_wcs_dict['wcs_info']['d_scale'] = active_wcs_dict['wcs_info']['scale'] - alt_wcs_dict['wcs_info']['scale']
                        diff_wcs_dict['wcs_info']['d_orientation'] = active_wcs_dict['wcs_info']['orientation'] - alt_wcs_dict['wcs_info']['orientation']
                        diff_wcs_dict['wcs_info']['exposure'] = edp_object.exposure_name

                        diagnostic_obj.add_data_item(diff_wcs_dict, 'Delta(Primary-Alternate)WCS_' + edp_object.exposure_name + '_' + str(icnt))

                        # Delete the original alternate WCS...
                        wcsutil.altwcs.deleteWCS(edp_object.full_filename, ext=extname_list, wcskey=alt_key)

                        # ... and archive the current primary with its original key
                        wcsutil.altwcs.archiveWCS(edp_object.full_filename, ext=extname_list, wcskey=alt_key)

                        icnt += 1

                    else:
                        continue

                # When comparisons are done between the original primary WCS and all
                # the alternates, restore the original primary WCS with key Z.
                wcsutil.altwcs.restoreWCS(edp_object.full_filename, ext=extname_list, wcskey='Z')

                # Delete the extra copy of the primary
                wcsutil.altwcs.deleteWCS(edp_object.full_filename, ext=extname_list, wcskey='Z')

                # Write out the results
                # diagnostic_obj.add_data_item(summary_dict, 'WCS Information Primary WCS - Alternate WCS')
            else:
                log.info("This dataset only has the Primary and OPUS WCS values")

        diagnostic_obj.write_json_file(json_filename)
        log.info("WCS information Primary WCS - Alternate WCS.".format(json_filename))

        # Clean up
        del diagnostic_obj

    # This routine does not return any values


# ----------------------------------------------------------------------------------------------------------------------


def run_hla_sourcelist_comparison(total_list, diagnostic_mode=False, json_timestamp=None,
                                  json_time_since_epoch=None, log_level=logutil.logging.NOTSET):
    """ This subroutine automates execution of drizzlepac/devutils/comparison_tools/compare_sourcelists.py to
    compare HAP-generated filter catalogs with their HLA classic counterparts.

    NOTE: In order for this subroutine to run, the following environment variables need to be set:
    - HLA_CLASSIC_BASEPATH
    - HLA_BUILD_VER

    Alternatively, if the HLA classic path is unavailable, The comparison can be run using locally stored HLA
    classic files. The relevant HLA classic imagery and sourcelist files must be placed in a subdirectory of
    the current working directory called 'hla_classic'.

    Parameters
    ----------
    total_list: list
        List of TotalProduct objects, one object per instrument/detector combination is
        a visit.  The TotalProduct objects are comprised of FilterProduct and ExposureProduct
        objects.

    diagnostic_mode : Boolean, optional.
        create intermediate diagnostic files? Default value is False.

    json_timestamp: str, optional
        Universal .json file generation date and time (local timezone) that will be used in the instantiation
        of the HapDiagnostic object. Format: MM/DD/YYYYTHH:MM:SS (Example: 05/04/2020T13:46:35). If not
        specified, default value is logical 'None'

    json_time_since_epoch : float
        Universal .json file generation time that will be used in the instantiation of the HapDiagnostic
        object. Format: Time (in seconds) elapsed since January 1, 1970, 00:00:00 (UTC). If not specified,
        default value is logical 'None'

    log_level : int, optional
        The desired level of verboseness in the log statements displayed on the screen and written to the
        .log file.  Default value is 'NOTSET'.

    RETURNS
    -------
    Nothing.
    """
    # get HLA classic path details from envroment variables
    hla_classic_basepath = os.getenv('HLA_CLASSIC_BASEPATH')
    hla_build_ver = os.getenv("HLA_BUILD_VER")
    for tot_obj in total_list:
        combo_comp_pdf_list = []
        if hla_classic_basepath and hla_build_ver and os.path.exists(hla_classic_basepath):
            hla_cassic_basepath = os.path.join(hla_classic_basepath, tot_obj.instrument, hla_build_ver)
            hla_classic_path = os.path.join(hla_cassic_basepath,
                                            tot_obj.prop_id,
                                            tot_obj.prop_id + "_" + tot_obj.obset_id)  # Generate path to HLA classic products
        elif os.path.exists(os.path.join(os.getcwd(), "hla_classic")):  # For local testing
            hla_classic_basepath = os.path.join(os.getcwd(), "hla_classic")
            hla_classic_path = hla_classic_basepath
        else:
            return  # bail out if HLA classic path can't be found.
        for filt_obj in tot_obj.fdp_list:
            hap_imgname = filt_obj.drizzle_filename
            hla_imgname = glob.glob("{}/{}{}_dr*.fits".format(hla_classic_path,
                                                              filt_obj.basename,
                                                              filt_obj.filters))[0]
            if not os.path.exists(hap_imgname) or not os.path.exists(hla_imgname):  # Skip filter if one or both of the images can't be found
                continue
            for hap_sourcelist_name in [filt_obj.point_cat_filename, filt_obj.segment_cat_filename]:
                if hap_sourcelist_name.endswith("point-cat.ecsv"):
                    hla_classic_cat_type = "dao"
                    plotfile_prefix = filt_obj.product_basename + "_point"
                else:
                    hla_classic_cat_type = "sex"
                    plotfile_prefix = filt_obj.product_basename + "_segment"
                if hla_classic_basepath and hla_build_ver and os.path.exists(hla_classic_basepath):
                    hla_sourcelist_name = "{}/logs/{}{}_{}phot.txt".format(hla_classic_path,
                                                                           filt_obj.basename,
                                                                           filt_obj.filters,
                                                                           hla_classic_cat_type)
                else:
                    hla_sourcelist_name = "{}/{}{}_{}phot.txt".format(hla_classic_path, filt_obj.basename,
                                                                      filt_obj.filters, hla_classic_cat_type)
                if not os.path.exists(hap_sourcelist_name) or not os.path.exists(hla_sourcelist_name):  # Skip catalog type if one or both of the catalogs can't be found
                    continue

                # convert HLA Classic RA and Dec values to HAP reference frame so the RA and Dec comparisons are correct
                updated_hla_sourcelist_name = correct_hla_classic_ra_dec(hla_sourcelist_name,
                                                                         hap_imgname,
                                                                         hla_classic_cat_type,
                                                                         log_level)
                log.info("HAP image:                   {}".format(os.path.basename(hap_imgname)))
                log.info("HLA Classic image:           {}".format(os.path.basename(hla_imgname)))
                log.info("HAP catalog:                 {}".format(os.path.basename(hap_sourcelist_name)))
                log.info("HLA Classic catalog:         {}".format(os.path.basename(updated_hla_sourcelist_name)))

                # once all file exist checks are passed, execute sourcelist comparision
                return_status = csl.comparesourcelists(slNames=[updated_hla_sourcelist_name,
                                                                hap_sourcelist_name],
                                                       imgNames=[hla_imgname, hap_imgname],
                                                       good_flag_sum=255,
                                                       plotGen="file",
                                                       plotfile_prefix=plotfile_prefix,
                                                       output_json_filename=hap_sourcelist_name.replace(".ecsv",
                                                                                                        "_svm_compare_sourcelists.json"),
                                                       verbose=True,
                                                       log_level=log_level,
                                                       json_timestamp=json_timestamp,
                                                       json_time_since_epoch=json_time_since_epoch,
                                                       debugMode=diagnostic_mode)
                combo_comp_pdf_filename = "{}_comparision_plots.pdf".format(plotfile_prefix)
                if os.path.exists(combo_comp_pdf_filename):
                    combo_comp_pdf_list.append(combo_comp_pdf_filename)
        if len(combo_comp_pdf_list) > 0:  # combine all plots generated by compare_sourcelists.py for this total object into a single pdf file
            total_combo_comp_pdf_filename = "{}_svm_comparision_plots.pdf".format(tot_obj.drizzle_filename[:-9].replace("_total", ""))
            csl.pdf_merger(total_combo_comp_pdf_filename, combo_comp_pdf_list)
            log.info("Sourcelist comparison plots saved to file {}.".format(total_combo_comp_pdf_filename))


# ----------------------------------------------------------------------------------------------------------------------

def correct_hla_classic_ra_dec(orig_hla_classic_sl_name, hap_imgname, cattype, log_level):
    """
    This subroutine runs Rick White's read_hla_catalog script to convert the RA and Dec values from a HLA Classic
    sourcelist into same reference frame used by the HAP sourcelists. Additionally, the new RA and Dec values
    are then transformed to produce X and Y coordinates that are in the HAP image frame of reference. A new
    version of the input file with the converted X and Y and RA and Dec values is written to the current
    working directory named <INPUT SOURCELIST NAME>_corrected.txt.

    Parameters
    ----------
    orig_hla_classic_sl_name : string
        name of the HLA Classic sourcelist whose RA and Dec values will be converted.

    hap_imgname : string
        name of HAP image. The WCS info from this image will be used to transform the updated RA and DEC
        values to new X and Y values in the this image's frame of reference

    cattype : string
        HLA Classic catalog type. Either 'sex' (source extractor) or 'dao' (DAOphot).

    log_level : int
        The desired level of verboseness in the log statements displayed on the screen and written to the .log file.

    Returns
    -------
    mod_sl_name : string
        Name of the new version of the input file with the converted RA and Dec values
    """
    try:
        mod_sl_name = os.path.basename(orig_hla_classic_sl_name)

        # Execute read_hla_catalog.read_hla_catalog() to convert RA and Dec values
        dataset = mod_sl_name.replace("_{}phot.txt".format(cattype), "")
        modcat = read_hla_catalog.read_hla_catalog(dataset, cattype=cattype, applyomega=True, multiwave=False,
                                                   verbose=True, trim=False, log_level=log_level)
        # sort catalog with updated RA, DEC values so that ordering is the same as the uncorrected table and everything maps correclty.
        if cattype == "dao":
            sortcoltitle = "ID"
            x_coltitle = "X-Center"
            y_coltitle = "Y-Center"
        if cattype == "sex":
            sortcoltitle = "NUMBER"
            x_coltitle = "X-IMAGE"
            y_coltitle = "Y-IMAGE"

        modcat.sort(sortcoltitle)

        # Identify RA and Dec column names in the new catalog table object
        for ra_col_title in ["ra", "RA", "ALPHA_J2000", "alpha_j2000"]:
            if ra_col_title in modcat.colnames:
                true_ra_col_title = ra_col_title
                log.debug("RA Col_name: {}".format(true_ra_col_title))
                break
        for dec_col_title in ["dec", "DEC", "Dec", "DELTA_J2000", "delta_j2000"]:
            if dec_col_title in modcat.colnames:
                true_dec_col_title = dec_col_title
                log.debug("DEC Col_name: {}".format(true_dec_col_title))
                break

        # transform new RA and Dec values into X and Y values in the HAP reference frame
        ra_dec_values = np.stack((modcat[true_ra_col_title], modcat[true_dec_col_title]), axis=1)
        new_xy_values = hla_flag_filter.rdtoxy(ra_dec_values, hap_imgname, '[sci,1]')

        # get HLA Classic sourcelist data, replace existing RA and Dec column data with the converted RA and Dec column data
        cat = Table.read(orig_hla_classic_sl_name, format='ascii')
        cat['RA'] = modcat[true_ra_col_title]
        cat['DEC'] = modcat[true_dec_col_title]

        # update existing X and Y values with new X and Y values transformed from new RA and Dec values.
        cat[x_coltitle] = new_xy_values[:, 0]
        cat[y_coltitle] = new_xy_values[:, 1]

        # Write updated version of HLA Classic catalog to current working directory
        mod_sl_name = mod_sl_name.replace(".txt", "_corrected.txt")
        log.info("Corrected version of HLA Classic file {} with new X, Y and RA, Dec values written to {}.".format(os.path.basename(orig_hla_classic_sl_name), mod_sl_name))
        cat.write(mod_sl_name, format="ascii.csv")
        return mod_sl_name

    except Exception:
        log.warning("There was a problem converting the RA and Dec values. Using original uncorrected HLA Classic sourcelist instead.")
        log.warning("Comparisons may be of questionable quality.")
        return orig_hla_classic_sl_name



# ----------------------------------------------------------------------------------------------------------------------
def run_quality_analysis(total_obj_list, run_compare_num_sources=True, run_find_gaia_sources=True,
                         run_compare_hla_sourcelists=True, run_compare_ra_dec_crossmatches=True,
                         run_characterize_gaia_distribution=True, run_compare_photometry=True,
                         run_report_wcs=True, log_level=logutil.logging.NOTSET):
    """Run the quality analysis functions

    Parameters
    ----------
    total_obj_list : list
        List of one or more HAP drizzlepac.hlautils.Product.TotalProduct object(s) to process

    run_compare_num_sources : bool, optional
        Run 'compare_num_sources' test? Default value is True.

    run_find_gaia_sources : bool, optional
        Run 'find_gaia_sources' test? Default value is True.

    run_compare_hla_sourcelists : bool, optional
        Run 'run_compare_sourcelists' test? Default value is True.

    run_compare_ra_dec_crossmatches : bool, optional
        Run 'compare_ra_dec_crossmatches' test? Default value is True.

    run_characterize_gaia_distribution : bool, optional
        Run 'characterize_gaia_distribution' test? Default value is True.

    run_compare_photometry : bool, optional
        Run 'compare_photometry' test? Default value is True.

    run_report_wcs : bool, optional
        Run 'report_wcs' test? Devault value is True.

    log_level : int, optional
        The desired level of verboseness in the log statements displayed on the screen and written to the
        .log file. Default value is 'NOTSET'.

    Returns
    -------
    Nothing.
    """
    log.setLevel(log_level)

    # generate a timestamp values that will be used to make creation time, creation date and epoch values
    # common to each json file
    json_timestamp = datetime.now().strftime("%m/%d/%YT%H:%M:%S")
    json_time_since_epoch = time.time()

    # Determine number of sources in Point and Segment catalogs
    if run_compare_num_sources:
        total_catalog_list = []
        total_drizzle_list = []
        for total_obj in total_obj_list:
            total_drizzle_list.append(total_obj.drizzle_filename)
            total_catalog_list.append(total_obj.point_cat_filename)
            total_catalog_list.append(total_obj.segment_cat_filename)
        compare_num_sources(total_catalog_list, total_drizzle_list, json_timestamp=json_timestamp,
                            json_time_since_epoch=json_time_since_epoch, log_level=log_level)

    # Identify the number of GAIA sources in final product footprints
    if run_find_gaia_sources:
        for total_obj in total_obj_list:
            find_gaia_sources(total_obj, json_timestamp=json_timestamp,
                              json_time_since_epoch=json_time_since_epoch, log_level=log_level)
            for filter_obj in total_obj.fdp_list:
                find_gaia_sources(filter_obj, json_timestamp=json_timestamp,
                                  json_time_since_epoch=json_time_since_epoch, log_level=log_level)
                for exp_obj in filter_obj.edp_list:
                    find_gaia_sources(exp_obj, json_timestamp=json_timestamp,
                                      json_time_since_epoch=json_time_since_epoch, log_level=log_level)

    # Compare HAP sourcelists to their HLA Classic counterparts
    if run_compare_hla_sourcelists:
        if log_level == logutil.logging.DEBUG:
            diag_mode = True
        else:
            diag_mode = False
        run_hla_sourcelist_comparison(total_obj_list,
                                      diagnostic_mode=diag_mode,
                                      json_timestamp=json_timestamp,
                                      json_time_since_epoch=json_time_since_epoch,
                                      log_level=log_level)

    # Get point/segment cross-match RA/Dec statistics
    if run_compare_ra_dec_crossmatches:
        for total_obj in total_obj_list:
            for filter_obj in total_obj.fdp_list:
                compare_ra_dec_crossmatches(filter_obj, json_timestamp=json_timestamp,
                                            json_time_since_epoch=json_time_since_epoch, log_level=log_level)

    # Statistically characterize GAIA distribution
    if run_characterize_gaia_distribution:
        for total_obj in total_obj_list:
            for filter_obj in total_obj.fdp_list:
                characterize_gaia_distribution(filter_obj, json_timestamp=json_timestamp,
                                               json_time_since_epoch=json_time_since_epoch,
                                               log_level=log_level)

    # Photometry of cross-matched sources in Point and Segment catalogs for Filter products
    if run_compare_photometry:
        tot_len = len(total_obj_list)
        filter_drizzle_list = []
        temp_list = []
        for tot in total_obj_list:
            temp_list = [x.drizzle_filename for x in tot.fdp_list]
            filter_drizzle_list.extend(temp_list)
        compare_photometry(filter_drizzle_list, json_timestamp=json_timestamp,
                           json_time_since_epoch=json_time_since_epoch, log_level=log_level)

    # Report WCS info
    if run_report_wcs:
        report_wcs(total_obj_list, json_timestamp=json_timestamp, json_time_since_epoch=json_time_since_epoch,
                   log_level=log_level)


# ============================================================================================================


if __name__ == "__main__":
    # process command-line inputs with argparse
    parser = argparse.ArgumentParser(description='Perform quality assessments of the SVM products generated '
                                                 'by the drizzlepac package. NOTE: if no QA switches (-cgd, '
                                                 '-cns, -cp, -cxm, or -fgs) are specified, ALL QA steps will '
                                                 'be executed.')
    parser.add_argument('input_filename', help='_total_list.pickle file that holds vital information about '
                                               'the processing run')
    parser.add_argument('-cgd', '--run_characterize_gaia_distribution', required=False, action='store_true',
                        help="Statistically describe distribution of GAIA sources in footprint.")
    parser.add_argument('-cns', '--run_compare_num_sources', required=False, action='store_true',
                        help='Determine the number of viable sources actually listed in SVM output catalogs.')
    parser.add_argument('-cp', '--run_compare_photometry', required=False, action='store_true',
                        help="Compare photometry measurements for sources cross matched between the Point "
                             "and Segment catalogs.")
    parser.add_argument('-cxm', '--run_compare_ra_dec_crossmatches', required=False, action='store_true',
                        help="Compare RA/Dec position measurements for sources cross matched between the "
                             "Point and Segment catalogs.")
    parser.add_argument('-fgs', '--run_find_gaia_sources', required=False, action='store_true',
                        help="Determine the number of GAIA sources in the footprint of a specified HAP final "
                             "product image")
    parser.add_argument('-hla', '--run_compare_hla_sourcelists', required=False, action='store_true',
                        help="Compare HAP sourcelists to their HLA classic counterparts")
    parser.add_argument('-wcs', '--run_report_wcs', required=False, action='store_true',
                        help="Report the WCS information for each exposure of a total data product")
    parser.add_argument('-l', '--log_level', required=False, default='info',
                        choices=['critical', 'error', 'warning', 'info', 'debug'],
                        help='The desired level of verboseness in the log statements displayed on the screen '
                             'and written to the .log file. The level of verboseness from left to right, and '
                             'includes all log statements with a log_level left of the specified level. '
                             'Specifying "critical" will only record/display "critical" log statements, and '
                             'specifying "error" will record/display both "error" and "critical" log '
                             'statements, and so on.')
    user_args = parser.parse_args()

    # set up logging
    log_dict = {"critical": logutil.logging.CRITICAL,
                "error": logutil.logging.ERROR,
                "warning": logutil.logging.WARNING,
                "info": logutil.logging.INFO,
                "debug": logutil.logging.DEBUG}
    log_level = log_dict[user_args.log_level]
    log.setLevel(log_level)

    # verify that input file exists
    if not os.path.exists(user_args.input_filename):
        err_msg = "File {} doesn't exist.".format(user_args.input_filename)
        log.critical(err_msg)
        raise Exception(err_msg)

    #  check that at least one QA switch is turned on
    all_qa_steps_off = True
    max_step_str_length = 0
    for kv_pair in user_args._get_kwargs():
        if kv_pair[0] not in ['input_filename', 'run_all', 'log_level']:
            if len(kv_pair[0])-4 > max_step_str_length:
                max_step_str_length = len(kv_pair[0])-4
            if kv_pair[1]:
                all_qa_steps_off = False

    # if no QA steps are explicitly turned on in the command-line call, run ALL the QA steps
    if all_qa_steps_off:
        log.info("No specific QA switches were turned on. All QA steps will be executed.")
        user_args.run_characterize_gaia_distribution = True
        user_args.run_compare_num_sources = True
        user_args.run_compare_photometry = True
        user_args.run_compare_hla_sourcelists = True
        user_args.run_compare_ra_dec_crossmatches = True
        user_args.run_find_gaia_sources = True
        user_args.run_report_wcs = True

    # display status summary indicating which QA steps are turned on and which steps are turned off
    log.info("{}QA step run status".format(" "*(int(max_step_str_length/2)-6)))
    for kv_pair in user_args._get_kwargs():
        if kv_pair[0] not in ['input_filename', 'run_all', 'log_level']:
            if kv_pair[1]:
                run_status = "ON"
            else:
                run_status = "off"
            log.info("{}{}   {}".format(kv_pair[0][4:], " "*(max_step_str_length-(len(kv_pair[0])-4)),
                                        run_status))
    log.info("-"*(max_step_str_length+6))

    # execute specified tests
    filehandler = open(user_args.input_filename, 'rb')
    total_obj_list = pickle.load(filehandler)
    run_quality_analysis(total_obj_list,
                         run_compare_num_sources=user_args.run_compare_num_sources,
                         run_find_gaia_sources=user_args.run_find_gaia_sources,
                         run_compare_ra_dec_crossmatches=user_args.run_compare_ra_dec_crossmatches,
                         run_characterize_gaia_distribution=user_args.run_characterize_gaia_distribution,
                         run_compare_hla_sourcelists=user_args.run_compare_hla_sourcelists,
                         run_compare_photometry=user_args.run_compare_photometry,
                         run_report_wcs=user_args.run_report_wcs,
                         log_level=log_level)
